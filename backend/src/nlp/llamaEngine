const { exec } = require('child_process');

class LlamaEngine {
  constructor() {
    this.modelName = 'llama2';
  }

  /**
   * Invia un messaggio al modello Llama tramite Ollama e restituisce la risposta.
   * @param {string} message - Il messaggio dell'utente.
   * @returns {Promise<string>} - La risposta generata dal modello Llama.
   */
  async processMessage(message) {
    if (!message) {
      throw new Error("Il messaggio Ã¨ obbligatorio.");
    }

    return new Promise((resolve, reject) => {
      const command = `echo "${message}" | ollama run ${this.modelName}`;
      console.log("Eseguo comando:", command);

      exec(command, (error, stdout, stderr) => {
        console.log("stdout:", stdout);
        console.log("stderr:", stderr);

        if (error) {
          console.error("Errore durante l'esecuzione di ollama:", error);
          reject(`Errore nell'esecuzione di ollama: ${error.message}`);
          return;
        }

        // Se stderr contiene avvisi, loggali come warning ma continua
        if (stderr) {
          console.warn("Avviso (stderr):", stderr);
        }

        // Verifica che stdout contenga una risposta valida
        if (!stdout) {
          console.error("Nessuna risposta da ollama.");
          reject("Nessuna risposta dal modello Llama2.");
          return;
        }

        resolve(stdout.trim());
      });
    });
  }
}

module.exports = LlamaEngine;
